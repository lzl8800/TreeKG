# -*- coding: utf-8 -*-
"""
HiddenKG ç¬¬ä¸€æ­¥ï¼šContextual-based Convolutionï¼ˆconvï¼‰

è¾“å‡ºï¼š
  - HiddenKG/output/conv_entities.json

æ—¥å¿—ï¼š
  - HiddenKG/logs/conv.log  ï¼ˆè¯¦ç»†ï¼‰
  - ç»ˆç«¯ï¼šä»…å¿…è¦ä¿¡æ¯ + è¿›åº¦æ¡

ä¾èµ–ï¼š
  - HiddenKG/config/config.yamlï¼ˆé€šè¿‡ include_files å¼•å…¥ config/conv.yamlï¼‰
  - éœ€è¦ config å†…å«:
      APIConfig: { API_BASE, API_KEY, MODEL_NAME, TIMEOUT_SECS(å¯é€‰) }
      ConvConfig: è§ conv.yaml
"""

from __future__ import annotations

import json
import time
import logging
import requests
import re
import os
import threading
from dataclasses import dataclass, asdict
from pathlib import Path
from typing import Dict, List, Tuple, Any, DefaultDict, Optional
from collections import defaultdict
from itertools import combinations
import yaml
from tqdm import tqdm
import sys

# ========== é…ç½®åŠ è½½ ==========
def load_config(config_file: Path) -> dict:
    with config_file.open("r", encoding="utf-8") as f:
        base = yaml.safe_load(f) or {}
    if "include_files" in base:
        merged = dict(base)
        for rel in base["include_files"]:
            inc_path = (config_file.parent / rel).resolve()
            with inc_path.open("r", encoding="utf-8") as ff:
                part = yaml.safe_load(ff) or {}
            merged.update(part)
        return merged
    return base

# å½“å‰æ¨¡å—ç›®å½•ï¼šsrc/HiddenKG
script_dir = Path(__file__).resolve().parent
config_file = script_dir / "config" / "config.yaml"
config = load_config(config_file)

APIConfig = config.get("APIConfig", {})
ConvConfig = config.get("ConvConfig", {})

# ç›®å½•
hidden_dir = script_dir                                 # src/HiddenKG
explicit_dir = script_dir.parent / "ExplicitKG"         # src/ExplicitKG
hidden_output_dir = hidden_dir / "output"
hidden_logs_dir = hidden_dir / "logs"
explicit_output_dir = explicit_dir / "output"
hidden_output_dir.mkdir(parents=True, exist_ok=True)
hidden_logs_dir.mkdir(parents=True, exist_ok=True)

# æ–‡ä»¶è·¯å¾„
FILE_TOC_ENT_REL = explicit_output_dir / ConvConfig.get("TOC_ENT_REL_NAME", "toc_with_entities_and_relations.json")
FILE_CONV_RESULT = hidden_output_dir / ConvConfig.get("CONV_RESULT_NAME", "conv_entities.json")
FILE_CONV_PROMPTS = hidden_output_dir / ConvConfig.get("CONV_PROMPTS_NAME", "conv_prompts.json")  # è‹¥åç»­éœ€è¦å¯å†™

# è¿è¡Œå‚æ•°
TEMPERATURE = float(ConvConfig.get("TEMPERATURE", 0.2))
MAX_TOKENS = int(ConvConfig.get("MAX_TOKENS", 300))
API_TIMEOUT = int(ConvConfig.get("API_TIMEOUT", 120))
RETRIES = int(ConvConfig.get("RETRIES", 3))
CHAT_COMPLETIONS_PATH = ConvConfig.get("CHAT_COMPLETIONS_PATH", "/chat/completions")
RATE_LIMIT_QPS = float(ConvConfig.get("RATE_LIMIT_QPS", 0))
EXTRA_THROTTLE_SEC = float(ConvConfig.get("EXTRA_THROTTLE_SEC", 0.0))
RETRY_BACKOFF_BASE = float(ConvConfig.get("RETRY_BACKOFF_BASE", 1.8))
MAX_NEIGHBORS_GLOBAL = int(ConvConfig.get("MAX_NEIGHBORS_GLOBAL", 30))
DRY_RUN = bool(ConvConfig.get("DRY_RUN", False))

# ========== æ—¥å¿— ==========
LOG_PATH = hidden_logs_dir / "conv.log"
# æ–‡ä»¶æ—¥å¿—ï¼šINFO çº§åˆ«ï¼ˆè¯¦ç»†ï¼‰
file_handler = logging.FileHandler(LOG_PATH, encoding="utf-8")
file_handler.setLevel(logging.INFO)
file_handler.setFormatter(logging.Formatter("%(asctime)s [%(levelname)s] %(message)s"))

# æ§åˆ¶å°æ—¥å¿—ï¼šWARNINGï¼ˆä»…å¿…è¦ä¿¡æ¯ï¼›è¿›åº¦æ¡ç”± tqdm è´Ÿè´£ï¼‰
console_handler = logging.StreamHandler(sys.stdout)
console_handler.setLevel(logging.WARNING)
console_handler.setFormatter(logging.Formatter("[%(levelname)s] %(message)s"))

logger = logging.getLogger("Conv")
logger.setLevel(logging.INFO)
# é˜²æ­¢é‡å¤æ·»åŠ 
if not logger.handlers:
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
else:
    logger.handlers.clear()
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)

# ========== æ•°æ®ç»“æ„ ==========
@dataclass
class Occurrence:
    path: str
    node_id: str
    level: int
    title: str

@dataclass
class Neighbor:
    name: str
    snippet: str = ""  # å…³ç³»æè¿°ï¼ˆåŒå°èŠ‚æ‘˜è¦æˆ–çŸ­æ–‡æœ¬ï¼‰

@dataclass
class EntityItem:
    name: str
    alias: List[str]
    type: str
    original: str
    occurrences: List[Occurrence]
    neighbors: List[Neighbor]
    updated_description: str = ""

# ========== TOC éå†ä¸å®ä½“è¯»å– ==========
def _iter_toc_nodes(toc: List[Dict[str, Any]], parent_path=""):
    for node in toc:
        title = node.get("title", "")
        node_id = node.get("id", "")
        level = node.get("level", -1)
        path = f"{parent_path} > {title}" if parent_path else title
        yield node, path, node_id, level
        for child in node.get("children", []) or []:
            yield from _iter_toc_nodes([child], path)

def load_entities_from_toc(file_path: Path) -> Dict[str, EntityItem]:
    """
    ä» TOC è½½å…¥å®ä½“ï¼Œå¹¶æ ¹æ®â€œåŒå°èŠ‚å…±ç°â€ç»Ÿè®¡å…¨å±€æƒé‡ï¼ŒæŒ‰æƒé‡æ’åºå†™å…¥ neighborsã€‚
    snippet ç»Ÿä¸€æ ¼å¼ï¼šcooccur|undirected|w=<count>|sec=<node_id>|title=<section_title>|raw=<trimmed_raw>
    ï¼ˆä¸æ–°å¢ dataclass å­—æ®µï¼Œå®Œå…¨å…¼å®¹åç»­ aggrï¼‰
    """
    with file_path.open("r", encoding="utf-8") as f:
        toc = json.load(f)

    entities: Dict[str, EntityItem] = {}
    # æ¯ä¸ªå¶å°èŠ‚ï¼š[(name, raw_content, section_title, section_id)]
    leaf_bucket: Dict[str, List[Tuple[str, str, str, str]]] = {}

    # 1) æ”¶é›†æ‰€æœ‰å¶å°èŠ‚é‡Œçš„å®ä½“
    for node, path, node_id, level in _iter_toc_nodes(toc):
        ents = node.get("entities") or []
        if not ents:
            continue
        sec_title = node.get("title", "")
        sec_id = node.get("id", "")
        triples: List[Tuple[str, str, str, str]] = []
        for e in ents:
            name = (e.get("name") or "").strip()
            if not name:
                continue
            raw = (e.get("raw_content") or "").strip()
            alias = e.get("alias") or []
            etype = e.get("type") or ""
            # å»ºå®ä½“
            if name not in entities:
                entities[name] = EntityItem(name, alias, etype, raw, [], [])
            # è®°å½•å‡ºç°ä½ç½®
            entities[name].occurrences.append(Occurrence(path, node_id, level, sec_title))
            # æ”¾è¿›è¯¥å¶å°èŠ‚çš„å®ä½“åˆ—è¡¨
            triples.append((name, raw, sec_title, sec_id))
        if triples:
            leaf_bucket.setdefault(node_id, []).extend(triples)

    # 2) ç»Ÿè®¡â€œå…¨å±€å…±ç°æ¬¡æ•°â€ï¼ˆè·¨å°èŠ‚ç´¯ç§¯ï¼‰
    from collections import defaultdict
    from itertools import combinations
    cooccur = defaultdict(int)  # key=(a,b) a<b
    # ä¹Ÿè®°å½•ï¼šä¸ºæŸä¸ª name->other çš„æœ€ä½³â€œç¤ºä¾‹å°èŠ‚â€ï¼ˆæ–¹ä¾¿å†™åˆ° snippet é‡Œï¼‰
    best_example: Dict[Tuple[str, str], Tuple[int, str, str, str]] = {}
    #    key=(name, other) â†’ (w, sec_id, sec_title, raw_other)

    for sec_id, triples in leaf_bucket.items():
        # å»é‡åŒä¸€å°èŠ‚é‡Œçš„åŒå
        uniq_names = {}
        for n, raw, s_title, s_id in triples:
            uniq_names[n] = (raw, s_title, s_id)
        names = sorted(uniq_names.keys())
        for a, b in combinations(names, 2):
            key = (a, b) if a < b else (b, a)
            cooccur[key] += 1

        # è®°å½•è¿™ä¸ªå°èŠ‚ä½œä¸º name-other çš„å€™é€‰â€œæœ€ä½³ç¤ºä¾‹â€
        for a in names:
            for b in names:
                if a == b:
                    continue
                raw_b, s_title, s_id = uniq_names[b]
                k = (a, b)
                w = cooccur[(a, b) if a < b else (b, a)]
                old = best_example.get(k)
                if (old is None) or (w > old[0]):  # ç”¨æ›´å¤§æƒé‡çš„å°èŠ‚åšç¤ºä¾‹
                    best_example[k] = (w, s_id, s_title, raw_b)

    # 3) ä¸ºæ¯ä¸ªå®ä½“æ„å»ºé‚»å±…ï¼ŒæŒ‰æƒé‡æ’åºï¼Œå†æˆªæ–­
    def _safe(s: str) -> str:
        # é¿å… '|' ç ´å snippet ç»“æ„
        return (s or "").replace("|", " ").replace("\n", " ").strip()

    for name, ent in entities.items():
        # æ”¶é›†æ‰€æœ‰ä¸ name å…±ç°è¿‡çš„ other
        touched = set()
        for (a, b), w in cooccur.items():
            if a == name:
                touched.add(b)
            elif b == name:
                touched.add(a)

        # å†™å…¥ neighborï¼Œå¸¦ä¸Š cooccur æƒé‡&ç¤ºä¾‹å°èŠ‚
        tmp_list = []
        for other in touched:
            key = (name, other)
            # æƒé‡æ¥è‡ªå¯¹ç§° key
            w = cooccur[(name, other) if name < other else (other, name)]
            # å–è¿™ä¸ªæ–¹å‘ä¸Šè®°å½•çš„â€œæœ€ä½³ç¤ºä¾‹å°èŠ‚â€
            w2, sec_id, sec_title, raw_other = best_example.get(key, (w, "", "", ""))
            w = max(w, w2)

            snippet = f"cooccur|undirected|w={w}|sec={_safe(sec_id)}|title={_safe(sec_title)}|raw={_safe(raw_other)[:80]}"
            tmp_list.append((w, other, snippet))

        # æ’åºï¼ˆæƒé‡å¤§ä¼˜å…ˆï¼Œå…¶æ¬¡æŒ‰åç§°ï¼‰
        tmp_list.sort(key=lambda t: (-t[0], t[1]))
        # æˆªæ–­ï¼ˆå»ºè®®æŠŠ MAX_NEIGHBORS_GLOBAL è°ƒå¤§ä¸€äº›ï¼Œä¾‹å¦‚ 200ï¼‰
        keep = tmp_list[:MAX_NEIGHBORS_GLOBAL] if MAX_NEIGHBORS_GLOBAL > 0 else tmp_list

        # èµ‹å€¼
        ent.neighbors = [Neighbor(name=o, snippet=snip) for _, o, snip in keep]

    # 4) å¯¹ç§°è¡¥é½ï¼ˆå¦‚æœ A æœ‰ Bï¼Œä½† B æ²¡æœ‰ Aï¼Œåˆ™è¡¥ä¸Šï¼‰
    for a, ent in entities.items():
        has = {n.name for n in ent.neighbors}
        for b in list(has):
            if a not in {n.name for n in entities[b].neighbors}:
                # ä¼°ç®—å¯¹åº” cooccur æƒé‡å’Œç¤ºä¾‹ï¼ˆåå‘ï¼‰
                from re import search
                w, sec_id, sec_title, raw_a = best_example.get((b, a), (1, "", "", ""))
                w = max(w, cooccur[(a, b) if a < b else (b, a)])
                snip = f"cooccur|undirected|w={w}|sec={_safe(sec_id)}|title={_safe(sec_title)}|raw={_safe(raw_a)[:80]}"
                entities[b].neighbors.append(Neighbor(name=a, snippet=snip))

        # å¦‚éœ€ä¹Ÿå¯¹è¡¥é½çš„é‚»å±…å†æˆªæ–­ï¼Œå¯åœ¨è¿™é‡Œå†æ¬¡æŒ‰ w æ’åºï¼ˆé€šè¿‡è§£æ snippet ä¸­ w=...ï¼‰
        if MAX_NEIGHBORS_GLOBAL > 0 and len(entities[a].neighbors) > MAX_NEIGHBORS_GLOBAL:
            def _w(n: Neighbor) -> int:
                m = re.search(r"w=(\d+)", n.snippet)
                return -(int(m.group(1)) if m else 1)
            entities[a].neighbors.sort(key=lambda n: (_w(n), n.name))
            entities[a].neighbors = entities[a].neighbors[:MAX_NEIGHBORS_GLOBAL]

    logger.info(f"[TOC] åŠ è½½å®ä½“ {len(entities)} ä¸ªï¼›æ¥è‡ªæ–‡ä»¶ï¼š{file_path}")
    return entities

# ========== Promptï¼ˆä»é…ç½®è¯»å–ï¼‰ ==========
def build_system_prompt() -> str:
    # ä» ConvConfig è¯»å–ï¼Œæä¾›ä¸æ—§ç‰ˆä¸€è‡´çš„é»˜è®¤å€¼
    return ConvConfig.get(
        "PROMPT_SYSTEM",
        "ä½ æ˜¯â€œMATLABç§‘å­¦è®¡ç®—è¯¾ç¨‹çŸ¥è¯†å›¾è°±ä¸“å®¶â€ã€‚è¯·åŸºäºé‚»åŸŸå…³ç³»å¢å¼ºå®ä½“æè¿°ï¼Œä¿æŒæœ¯è¯­å‡†ç¡®ï¼Œä¸­æ–‡è¾“å‡ºï¼Œ200â€“300å­—ä»¥å†…ã€‚"
    )

def build_user_prompt(entity: EntityItem) -> str:
    # å°†é‚»å±…ä¸å‡ºç°ä½ç½®æ‹¼æ¥ä¸ºæ¨¡æ¿æ‰€éœ€å˜é‡ï¼Œç„¶åå¥—å…¥ ConvConfig çš„æ¨¡æ¿
    neighbor_lines = []
    for nb in entity.neighbors:
        snip = (nb.snippet or "").replace("\n", " ").strip()
        neighbor_lines.append(f"- {nb.name}ï¼š{snip}" if snip else f"- {nb.name}")
    neighbors_block = "\n".join(neighbor_lines) if neighbor_lines else "ï¼ˆæ— ï¼‰"

    occ_str = "; ".join([o.path for o in entity.occurrences[:3]]) or "ï¼ˆæ— ï¼‰"

    tpl = ConvConfig.get("PROMPT_USER_TEMPLATE")
    if not tpl:
        # å…œåº•æ¨¡æ¿ï¼šä¸å†å²é€»è¾‘ç­‰ä»·
        tpl = (
            "ç›®æ ‡å®ä½“ï¼š{entity_name}\n"
            "åŸå§‹æè¿°ï¼š{original_desc}\n\n"
            "å‡ºç°ä½ç½®ï¼ˆæœ€å¤š3å¤„ï¼‰ï¼š{occurrences}\n\n"
            "é‚»åŸŸå®ä½“ï¼š\n{neighbors}\n\n"
            "ä»»åŠ¡ï¼šåŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆå¢å¼ºæè¿°ï¼Œèšç„¦å…¶å®šä¹‰ã€ä½œç”¨åŠä¸é‚»åŸŸçš„å…³ç³»ã€‚\n"
            "è¾“å‡ºè¦æ±‚ï¼šä»…è¿”å›JSONï¼Œä¸æ·»åŠ ä»»ä½•å¤šä½™æ–‡å­—ï¼ˆå¦‚è§£é‡Šã€è¯´æ˜ã€æ¢è¡Œï¼‰ï¼\n"
            "è¯·è¿”å›JSONï¼š{\"name\": \"å®ä½“å\", \"updated_description\": \"å¢å¼ºåçš„æè¿°\"}"
        )

    return tpl.format(
        entity_name=entity.name,
        original_desc=entity.original or "ï¼ˆæ— æè¿°ï¼‰",
        occurrences=occ_str,
        neighbors=neighbors_block
    )

# ========== é™é€Ÿ ==========
_rate_lock = threading.Lock()
_last_ts = 0.0
_min_interval = (1.0 / RATE_LIMIT_QPS) if RATE_LIMIT_QPS > 0 else 0.0

def _rate_limit_block():
    if _min_interval <= 0:
        return
    global _last_ts
    with _rate_lock:
        now = time.time()
        delta = now - _last_ts
        if delta < _min_interval:
            time.sleep(_min_interval - delta)
        _last_ts = time.time()

# ========== LLM è°ƒç”¨ ==========
def call_llm(system_prompt: str, user_prompt: str) -> str:
    """è°ƒç”¨ OpenAI å…¼å®¹ /chat/completionsï¼›æ”¯æŒæ— é‰´æƒæœ¬åœ°ç½‘å…³"""
    if DRY_RUN:
        return ""

    api_base = APIConfig.get("API_BASE", "")
    if not api_base:
        logger.warning("API_BASE ä¸ºç©ºï¼Œè·³è¿‡ LLM è°ƒç”¨ã€‚")
        return ""

    headers = {"Content-Type": "application/json"}
    api_key = APIConfig.get("API_KEY")
    if api_key:
        headers["Authorization"] = f"Bearer {api_key}"

    payload = {
        "model": APIConfig.get("MODEL_NAME", ""),
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        "temperature": TEMPERATURE,
        "max_tokens": MAX_TOKENS
    }

    last_err: Optional[Exception] = None
    for k in range(RETRIES):
        try:
            _rate_limit_block()
            if EXTRA_THROTTLE_SEC > 0:
                time.sleep(EXTRA_THROTTLE_SEC)

            resp = requests.post(
                f"{api_base}{CHAT_COMPLETIONS_PATH}",
                headers=headers,
                json=payload,
                timeout=API_TIMEOUT
            )
            resp.raise_for_status()
            text = resp.json()["choices"][0]["message"]["content"].strip()
            text = re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL)
            text = text.strip()
            return text
        except Exception as e:
            last_err = e
            logger.warning(f"LLM è°ƒç”¨å¤±è´¥({k+1}/{RETRIES})ï¼š{e}")
            time.sleep(RETRY_BACKOFF_BASE ** k)
    logger.error(f"LLM è¯·æ±‚å¤±è´¥ï¼š{last_err}")
    return ""

# ========== è§£æä¸æ¸…æ´— ==========
def safe_json_loads(txt: str) -> Dict[str, Any]:
    if not txt or not isinstance(txt, str):
        return {}
    txt = txt.strip()
    try:
        return json.loads(txt)
    except Exception:
        pass
    candidates = re.findall(r"\{[^{}]*\}", txt)
    for c in candidates:
        try:
            obj = json.loads(c)
            if isinstance(obj, dict) and "name" in obj:
                return obj
        except Exception:
            continue
    start, end = txt.find("{"), txt.rfind("}")
    if start != -1 and end != -1 and end > start:
        segment = txt[start:end + 1]
        try:
            return json.loads(segment)
        except Exception:
            cleaned = re.sub(r"^[^{]*|[^}]*$", "", segment)
            try:
                return json.loads(cleaned)
            except Exception:
                return {}
    return {}

def _enforce_len_200_300(s: str) -> str:
    s = (s or "").replace("\n", " ").strip()
    if len(s) > 300:
        s = s[:300]
        for c in "ã€‚ï¼›ï¼Œï¼‰ã€‘":
            pos = s.rfind(c)
            if 200 <= pos <= 300:
                return s[:pos+1]
    return s

# ========== ä¸»æµç¨‹ ==========
def run_conv():
    if not FILE_TOC_ENT_REL.exists():
        raise FileNotFoundError(
            f"æœªæ‰¾åˆ° toc_with_entities_and_relations.jsonï¼š{FILE_TOC_ENT_REL}\n"
            f"ï¼ˆæ–‡ä»¶ååœ¨ ConvConfig.TOC_ENT_REL_NAMEï¼›è·¯å¾„å·²è‡ªåŠ¨æ‹¼æ¥ä¸º ExplicitKG/output ä¸‹ï¼‰"
        )

    print(f"â–¶ å¼€å§‹ Convï¼š{FILE_TOC_ENT_REL.name}")
    logger.info("å¼€å§‹è¿è¡Œ Contextual-based Convolutionï¼ˆconvï¼‰é˜¶æ®µ")
    logger.info(f"è¾“å…¥æ–‡ä»¶: {FILE_TOC_ENT_REL}")

    # 1) åŠ è½½å®ä½“ä¸é‚»å±…ï¼ˆä»…åŒå°èŠ‚å…±ç°ï¼‰
    entities = load_entities_from_toc(FILE_TOC_ENT_REL)

    # 2) ç”Ÿæˆå¢å¼ºæè¿°
    system_prompt = build_system_prompt()
    results: Dict[str, Any] = {}

    limit = int(os.getenv("CONV_LIMIT", "0")) or None
    items = list(entities.items())[:limit] if limit else list(entities.items())
    total = len(items)
    start_ts = time.time()
    logger.info(f"å…±åŠ è½½ {total} ä¸ªå®ä½“ï¼Œå¼€å§‹è°ƒç”¨ LLM ç”Ÿæˆå¢å¼ºæè¿°...")

    # è¿›åº¦æ¡ï¼ˆåªåœ¨ç»ˆç«¯å±•ç¤ºï¼‰
    for name, item in tqdm(items, desc="LLM ç”Ÿæˆå¢å¼ºæè¿°", ncols=90, file=sys.stdout):
        user_prompt = build_user_prompt(item)
        content = call_llm(system_prompt, user_prompt)
        obj = safe_json_loads(content)
        upd = (obj.get("updated_description") or "").strip() or item.original or f"{name}ï¼šæš‚æ— æè¿°ã€‚"
        upd = _enforce_len_200_300(upd)
        item.updated_description = upd

        results[name] = {
            "name": name,
            "alias": item.alias,
            "type": item.type,
            "original": item.original,
            "updated_description": upd,
            "occurrences": [asdict(o) for o in item.occurrences],
            "neighbors": [asdict(nb) for nb in item.neighbors]
        }
        logger.info(f"[conv] {name} ç”ŸæˆæˆåŠŸã€‚")

    # 3) è¾“å‡ºç»“æœ
    FILE_CONV_RESULT.parent.mkdir(parents=True, exist_ok=True)
    with FILE_CONV_RESULT.open("w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    duration = round(time.time() - start_ts, 2)
    logger.info(f"Conv é˜¶æ®µå®Œæˆï¼Œå…±å¤„ç† {len(results)} ä¸ªå®ä½“ã€‚")
    logger.info(f"è¾“å‡ºæ–‡ä»¶: {FILE_CONV_RESULT}")
    logger.info(f"æ€»è€—æ—¶: {duration} ç§’")
    print(f"âœ… Conv å®Œæˆï¼š{len(results)} ä¸ªå®ä½“ï¼Œè€—æ—¶ {duration} s  â†’  {FILE_CONV_RESULT}")
    print(f"ğŸ“ è¯¦ç»†æ—¥å¿—ï¼š{LOG_PATH}")

# ========== CLI ==========
if __name__ == "__main__":
    run_conv()
