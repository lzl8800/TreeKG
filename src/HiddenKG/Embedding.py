# -*- coding: utf-8 -*-
"""
Embedding.py
ä½¿ç”¨æŠ½è±¡æ¨¡å‹æ¥å£ç”Ÿæˆå®ä½“åµŒå…¥ï¼ˆæ”¯æŒæ‰¹å¤„ç†ã€AMPã€L2å½’ä¸€åŒ–ã€PKLå­˜å‚¨ï¼‰
"""

import os
import json
import pickle
import argparse
from typing import Dict, List
import numpy as np

# ===== å¯¼å…¥é…ç½®ä¸æ¨¡å‹å…¥å£ =====
try:
    from HiddenKG.config import Emb as EmbConfig
    from HiddenKG.model.main import get_encoder
except Exception:
    EmbConfig = None  # type: ignore
    from model.main import get_encoder  # ç‹¬ç«‹è¿è¡Œæ—¶è·¯å¾„å…œåº•


# ================ å®ä½“è¯»å–ä¸æ–‡æœ¬æå– ================
def load_entities(file_path: str) -> Dict[str, dict]:
    with open(file_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    print(f"ğŸ“˜ å·²åŠ è½½å®ä½“ {len(data)} ä¸ªï¼š{os.path.basename(file_path)}")
    return data


def entity_text(e: dict) -> str:
    return (e.get("updated_description") or e.get("original") or "").strip() or "[EMPTY]"


# ================ ç”ŸæˆåµŒå…¥ ================
def generate_embeddings(encoder, entities: Dict[str, dict]) -> Dict[str, np.ndarray]:
    items = sorted(entities.items(), key=lambda kv: kv[0])
    names: List[str] = [k for k, _ in items]
    texts: List[str] = [entity_text(v) for _, v in items]

    mat = encoder.encode_texts(texts)
    name_to_vec = {n: mat[i] for i, n in enumerate(names)}

    print(f"âœ… å·²ç”ŸæˆåµŒå…¥å‘é‡ï¼š{mat.shape[0]} Ã— {mat.shape[1]}")
    return name_to_vec


# ================ ä¿å­˜ ================
def save_embeddings_pkl(name_to_vec: Dict[str, np.ndarray], out_file: str):
    os.makedirs(os.path.dirname(out_file), exist_ok=True)
    with open(out_file, "wb") as f:
        pickle.dump(name_to_vec, f)
    print(f"ğŸ’¾ å·²ä¿å­˜åµŒå…¥æ–‡ä»¶ï¼š{out_file}")


# ================ ä¸»å…¥å£ ================
def main():
    # ä»é…ç½®åŠ è½½é»˜è®¤å‚æ•°
    if EmbConfig:
        EmbConfig.ensure_paths()
        infile = os.fspath(EmbConfig.INPUT_FILE)
        outfile = os.fspath(EmbConfig.OUTPUT_FILE)
        batch_size = EmbConfig.BATCH_SIZE
        max_length = EmbConfig.MAX_LENGTH
        use_amp = EmbConfig.USE_AMP
        model_dir = os.fspath(EmbConfig.BERT_BASE_DIR)
        device = EmbConfig.DEVICE
    else:
        infile = os.getenv("EMB_INPUT", "HiddenKG/output/conv_entities.json")
        outfile = os.getenv("EMB_OUTPUT", "HiddenKG/output/node_embeddings.pkl")
        batch_size = int(os.getenv("EMB_BATCH_SIZE", "16"))
        max_length = int(os.getenv("EMB_MAX_LENGTH", "512"))
        use_amp = bool(int(os.getenv("EMB_USE_AMP", "1")))
        model_dir = os.getenv("EMB_BERT_DIR", "bert-base-chinese")
        device = os.getenv("EMB_DEVICE", "cuda")

    parser = argparse.ArgumentParser(description="ç”Ÿæˆå®ä½“åµŒå…¥å‘é‡")
    parser.add_argument("--infile", type=str, default=infile)
    parser.add_argument("--outfile", type=str, default=outfile)
    parser.add_argument("--batch_size", type=int, default=batch_size)
    parser.add_argument("--max_length", type=int, default=max_length)
    parser.add_argument("--no_amp", action="store_true")
    args = parser.parse_args()

    # åˆå§‹åŒ–æ¨¡å‹
    encoder = get_encoder(
        model_name=model_dir,
        device=device,
        batch_size=args.batch_size,
        max_length=args.max_length,
        use_amp=not args.no_amp and use_amp,
    )

    # ç”Ÿæˆå¹¶ä¿å­˜
    entities = load_entities(args.infile)
    name_to_vec = generate_embeddings(encoder, entities)
    save_embeddings_pkl(name_to_vec, args.outfile)


if __name__ == "__main__":
    main()
