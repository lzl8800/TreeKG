# -*- coding: utf-8 -*-
"""
Dedup.py â€” Tree-KG Â§3.3.4 å®ä½“å»é‡ï¼ˆçº¯ YAML é…ç½®ç‰ˆï¼‰

- ç»Ÿä¸€ä» HiddenKG/config/config.yaml è¯»å–ä¸»é…ç½®ï¼Œå¹¶è§£æ include_filesï¼ˆç›¸å¯¹ä¸»é…ç½®ç›®å½•ï¼‰
- æ‰€æœ‰è·¯å¾„åªå­˜â€œæ–‡ä»¶åâ€ï¼Œåœ¨ä»£ç é‡Œç»Ÿä¸€æ‹¼åˆ° HiddenKG/output ä¸‹
- ä¼˜å…ˆä½¿ç”¨ aggr çš„å®ä½“ï¼›è‹¥ä¸å­˜åœ¨åˆ™å›é€€åˆ° conv çš„å®ä½“
- ç»“æœè¾“å‡ºä¸º HiddenKG/output/<RESULT_NAME>
"""

import os
import re
import json
import time
import pickle
import logging
import argparse
from typing import Dict, List, Tuple, Any
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path

import numpy as np
from tqdm import tqdm
import yaml

# === å¤–éƒ¨æ¨¡å—ï¼ˆä¿æŒä¸å˜ï¼‰ ===
from Dedup.data_structures import Occurrence, Neighbor, EntityItem
from Dedup.knn import knn_topk
from Dedup.llm import llm_is_same, _normalize_name
from Dedup.name_similarity import _name_jaccard_mode


# =========================
# é…ç½®åŠ è½½ï¼ˆä¸»é…ç½® + include_filesï¼‰
# =========================
def load_config_with_includes(main_cfg_path: Path) -> Dict[str, Any]:
    with main_cfg_path.open("r", encoding="utf-8") as f:
        base = yaml.safe_load(f) or {}
    includes = base.get("include_files", []) or []
    merged = dict(base)
    for rel in includes:
        inc = (main_cfg_path.parent / rel).resolve()
        with inc.open("r", encoding="utf-8") as ff:
            sub = yaml.safe_load(ff) or {}
        merged.update(sub)
    return merged


# =========================
# I/O å·¥å…·å‡½æ•°
# =========================
def read_entities(infile: str) -> Dict[str, EntityItem]:
    with open(infile, "r", encoding="utf-8") as f:
        raw = json.load(f)
    entities: Dict[str, EntityItem] = {}
    for name, d in raw.items():
        entities[name] = EntityItem(
            name=name,
            alias=d.get("alias", []),
            type=d.get("type", ""),
            original=d.get("original", ""),
            updated_description=d.get("updated_description", ""),
            role=d.get("role", d.get("local_role", "")),
            occurrences=[Occurrence(**o) for o in d.get("occurrences", [])],
            neighbors=[Neighbor(**n) for n in d.get("neighbors", [])],
        )
    logging.info(f"è½½å…¥å®ä½“ {len(entities)} ä¸ªï¼š{os.path.basename(infile)}")
    return entities


def read_embeddings(pkl_path: str) -> Tuple[List[str], np.ndarray]:
    with open(pkl_path, "rb") as f:
        emb_dict = pickle.load(f)
    names = list(emb_dict.keys())
    embs = np.vstack([emb_dict[n] for n in names]).astype("float32")
    logging.info(f"è½½å…¥åµŒå…¥ {embs.shape[0]} x {embs.shape[1]}ï¼š{os.path.basename(pkl_path)}")
    return names, embs


# =========================
# å¹¶æŸ¥é›†
# =========================
class DSU:
    def __init__(self, n: int):
        self.fa = list(range(n))
        self.sz = [1] * n

    def find(self, x: int) -> int:
        while self.fa[x] != x:
            self.fa[x] = self.fa[self.fa[x]]
            x = self.fa[x]
        return x

    def union(self, a: int, b: int) -> bool:
        ra, rb = self.find(a), self.find(b)
        if ra == rb:
            return False
        if self.sz[ra] < self.sz[rb]:
            ra, rb = rb, ra
        self.fa[rb] = ra
        self.sz[ra] += self.sz[rb]
        return True


# =========================
# ä¸»æµç¨‹
# =========================
def run():
    # â€”â€” è·¯å¾„æ ¹ï¼šHiddenKG/ â€”â€” #
    script_dir = Path(__file__).resolve().parent
    cfg_path = script_dir / "config" / "config.yaml"
    cfg = load_config_with_includes(cfg_path)

    # â€”â€” å–å­é…ç½® â€”â€” #
    api = cfg["APIConfig"]                # è‹¥ Dedup.llm ç”¨å¾—åˆ°ï¼Œå¯åœ¨è¯¥æ¨¡å—å†…éƒ¨è¯»å–ä½ çš„ YAML
    dcfg = cfg["DedupConfig"]

    # â€”â€” ç›®å½•ä¸æ–‡ä»¶å â€”â€” #
    output_dir = script_dir / "output"
    output_dir.mkdir(parents=True, exist_ok=True)

    ent_aggr = output_dir / dcfg["ENTITIES_AGGR_NAME"]
    ent_conv = output_dir / dcfg["ENTITIES_CONV_NAME"]
    emb_path = output_dir / dcfg["EMBEDDINGS_NAME"]
    out_path = output_dir / dcfg["RESULT_NAME"]
    enc = dcfg.get("ENCODING", "utf-8")

    # â€”â€” å®ä½“æ–‡ä»¶ä¼˜å…ˆçº§ï¼šaggr > conv â€”â€” #
    if ent_aggr.exists():
        entities_path = ent_aggr
    elif ent_conv.exists():
        entities_path = ent_conv
    else:
        raise FileNotFoundError(
            f"æœªæ‰¾åˆ°å®ä½“æ–‡ä»¶ï¼š\n 1) {ent_aggr}\n 2) {ent_conv}\nï¼ˆè‡³å°‘å­˜åœ¨ä¸€ä¸ªï¼‰"
        )

    if not emb_path.exists():
        raise FileNotFoundError(f"æœªæ‰¾åˆ°åµŒå…¥æ–‡ä»¶ï¼š{emb_path}")

    # â€”â€” æ—¥å¿— â€”â€” #
    logging.basicConfig(
        level=logging.INFO,
        format="%(levelname)s: %(message)s",
        handlers=[logging.StreamHandler()]
    )
    logger = logging.getLogger("Dedup")

    t0 = time.time()
    logger.info("ğŸ“˜ [1/5] åŠ è½½å®ä½“ä¸åµŒå…¥æ–‡ä»¶...")
    entities = read_entities(str(entities_path))
    names_all, embs_all = read_embeddings(str(emb_path))
    name2idx = {n: i for i, n in enumerate(names_all)}

    # å¯¹é½å®ä½“
    names = [n for n in names_all if n in entities]
    if not names:
        raise ValueError("å®ä½“ä¸åµŒå…¥æ–‡ä»¶æ— äº¤é›†ï¼Œè¯·æ£€æŸ¥è¾“å…¥æ–‡ä»¶")
    idx_keep = np.array([name2idx[n] for n in names], dtype=np.int64)
    embs = embs_all[idx_keep].astype("float32")
    # L2 å½’ä¸€åŒ–
    embs /= (np.linalg.norm(embs, axis=1, keepdims=True) + 1e-12)
    logger.info(f"âœ… å¯¹é½åå‚ä¸å»é‡çš„å®ä½“æ•°ï¼š{len(names)}")

    # â€”â€” KNN â€”â€” #
    logger.info("ğŸ“˜ [2/5] å¼€å§‹è¿‘é‚»æ£€ç´¢ï¼ˆFAISS/Sklearnï¼‰...")
    topk = int(dcfg["KNN_NEIGHBORS"]) + 1
    dists, idxs = knn_topk(embs, topk=topk)

    ents: List[EntityItem] = [entities[n] for n in names]
    roles = [e.role or "" for e in ents]
    dsu = DSU(len(names))
    N = len(names)
    nbr_sets = [set(idxs[i, 1:]) for i in range(N)]

    # â€”â€” è§„åˆ™å‚æ•° â€”â€” #
    dist_thresh       = float(dcfg["DIST_THRESHOLD"])
    mut_knn           = bool(dcfg["MUTUAL_KNN"])
    rank_top          = int(dcfg["RANK_TOP"])
    per_node_cap      = int(dcfg["PER_NODE_CAP"])
    strict_same_role  = bool(dcfg["STRICT_SAME_ROLE"])
    use_name_jacc     = bool(dcfg["USE_NAME_JACCARD"])
    name_jacc_min     = float(dcfg["NAME_JACCARD_MIN"])
    name_mode         = str(dcfg["NAME_MODE"])
    use_alias_fast    = bool(dcfg["USE_ALIAS_FASTPATH"])
    trunc_desc        = bool(dcfg["TRUNCATE_DESC"])
    desc_maxlen       = int(dcfg["DESC_MAXLEN"])
    workers           = int(dcfg["LLM_WORKERS"])

    # â€”â€” å€™é€‰è¿‡æ»¤ â€”â€” #
    logger.info("ğŸ“˜ [3/5] å€™é€‰å¯¹è¿‡æ»¤ï¼ˆè·ç¦»é˜ˆå€¼ + è§’è‰² + åç§°Jaccard + äº’è¿‘é‚» + æ’å/é…é¢ï¼‰...")
    candidates: List[Tuple[int, int, float]] = []
    cnt_pairs = 0
    used = [0] * N

    rej_dist = rej_role = rej_jac = rej_mutual = rej_cap = 0
    rank_limit = max(2, rank_top)

    for i in range(N):
        for rank in range(1, min(idxs.shape[1], rank_limit)):
            j = idxs[i, rank]
            if j <= i or j >= N:
                continue
            cnt_pairs += 1
            dist = float(dists[i, rank])

            if dist >= dist_thresh:
                rej_dist += 1
                continue

            if strict_same_role:
                if roles[i] != roles[j]:
                    rej_role += 1
                    continue
            else:
                if roles[i] and roles[j] and roles[i] != roles[j]:
                    rej_role += 1
                    continue

            if use_name_jacc:
                jac = _name_jaccard_mode(ents[i].name, ents[j].name, name_mode)
                if jac < name_jacc_min:
                    rej_jac += 1
                    continue

            if mut_knn and (i not in nbr_sets[j]):
                rej_mutual += 1
                continue

            if per_node_cap > 0 and (used[i] >= per_node_cap or used[j] >= per_node_cap):
                rej_cap += 1
                continue

            candidates.append((i, j, dist))
            used[i] += 1
            used[j] += 1

    logger.info(f"ğŸ“Œ å€™é€‰å¯¹ï¼ˆé€šè¿‡åˆç­›ï¼‰ï¼š{len(candidates)} / åŸå§‹å¯¹æ•°ï¼š{cnt_pairs}")
    logger.info(f"[è¯Šæ–­] è¿‡æ»¤ç»Ÿè®¡: è·ç¦»={rej_dist} è§’è‰²={rej_role} åç§°Jaccard={rej_jac} äº’è¿‘é‚»={rej_mutual} é…é¢={rej_cap}")
    if candidates:
        kept_d = np.array([d for (_, _, d) in candidates], dtype=np.float32)
        qs = np.percentile(kept_d, [5, 10, 25, 50, 75, 90, 95])
        logger.info(f"[è¯Šæ–­] åˆç­›è·ç¦»åˆ†ä½æ•° p5..p95: {np.round(qs, 4)}")

    # â€”â€” åˆ«åå¿«é€Ÿåˆå¹¶ï¼ˆå¯é€‰ï¼‰ â€”â€” #
    rep_fast_merge = 0
    kept_for_llm: List[Tuple[int, int, float]] = candidates
    if use_alias_fast:
        logger.info("ğŸ“˜ [3.5] å¿«é€Ÿé€šé“ï¼šåˆ«å/åç§°äº¤é›†ç›´æ¥åˆå¹¶ï¼ˆå¯é€‰ï¼‰...")

        def alias_intersect(a: EntityItem, b: EntityItem) -> bool:
            alias_a = {_normalize_name(x) for x in ([a.name] + a.alias)}
            alias_b = {_normalize_name(x) for x in ([b.name] + b.alias)}
            return len(alias_a & alias_b) > 0

        tmp_kept: List[Tuple[int, int, float]] = []
        for (i, j, dist) in candidates:
            if alias_intersect(ents[i], ents[j]):
                if dsu.union(i, j):
                    rep_fast_merge += 1
            else:
                tmp_kept.append((i, j, dist))
        kept_for_llm = tmp_kept
        logger.info(f"âœ… å¿«é€Ÿåˆå¹¶å®Œæˆï¼š{rep_fast_merge} æ¡ï¼›è¿›å…¥ LLM çš„å¯¹æ•°ï¼š{len(kept_for_llm)}")
    else:
        logger.info(f"ğŸ“˜ [3.5] è·³è¿‡å¿«é€Ÿé€šé“ï¼›è¿›å…¥ LLM çš„å¯¹æ•°ï¼š{len(kept_for_llm)}")

    # â€”â€” LLM åˆ¤å®šï¼ˆå¹¶è¡Œï¼‰ â€”â€” #
    logger.info(f"ğŸ“˜ [4/5] LLM å¹¶è¡Œåˆ¤å®šï¼ˆå¹¶å‘={workers}ï¼›æŒ‰è·ç¦»å‡åºï¼‰...")
    kept_for_llm.sort(key=lambda x: x[2])
    llm_cache: Dict[Tuple[str, str], Tuple[bool, str]] = {}
    cnt_llm_calls = cnt_fallback = rep_merged = 0
    rep_merged += rep_fast_merge
    merge_decisions: List[Tuple[str, str, bool, float, str]] = []

    def judge_pair(i: int, j: int, dist: float) -> Tuple[int, int, bool, float, str, bool]:
        ent_i, ent_j = ents[i], ents[j]
        key = tuple(sorted((ent_i.name, ent_j.name)))
        if key in llm_cache:
            is_same, reason = llm_cache[key]
            return (i, j, is_same, dist, reason, False)
        is_same, reason, used_fallback = llm_is_same(
            ent_i, ent_j, truncate_desc=trunc_desc, desc_maxlen=desc_maxlen
        )
        llm_cache[key] = (is_same, reason)
        return (i, j, is_same, dist, reason, used_fallback)

    with ThreadPoolExecutor(max_workers=max(1, workers)) as ex:
        futures = [ex.submit(judge_pair, i, j, dist) for (i, j, dist) in kept_for_llm]
        for fut in tqdm(as_completed(futures), total=len(futures), desc="LLMåˆ¤å®šä¸­", ncols=80):
            i, j, is_same, dist, reason, used_fallback = fut.result()
            if used_fallback:
                cnt_fallback += 1
            else:
                cnt_llm_calls += 1
            merge_decisions.append((ents[i].name, ents[j].name, is_same, dist, reason))
            if is_same and dsu.union(i, j):
                rep_merged += 1

    # â€”â€” èšç±»ä¸åˆå¹¶ â€”â€” #
    logger.info("ğŸ“˜ [5/5] æ„å»ºç­‰ä»·ç±»ã€åˆå¹¶å¹¶å†™å‡ºç»“æœ...")
    clusters: Dict[int, List[int]] = {}
    for x in range(len(names)):
        root = dsu.find(x)
        clusters.setdefault(root, []).append(x)

    def desc_len(e: EntityItem) -> int:
        return len((e.updated_description or e.original or "").strip())

    def merge_two(base: EntityItem, other: EntityItem) -> None:
        merged_alias = list(dict.fromkeys(base.alias + other.alias + [other.name]))
        base.alias = [a for a in merged_alias if a != base.name]
        occ_seen = {(o.path, o.node_id) for o in base.occurrences}
        for o in other.occurrences:
            key = (o.path, o.node_id)
            if key not in occ_seen:
                base.occurrences.append(o)
                occ_seen.add(key)
        nb_seen = {(n.name, n.snippet) for n in base.neighbors}
        for n in other.neighbors:
            if n.name == base.name:
                continue
            key = (n.name, n.snippet)
            if key not in nb_seen:
                base.neighbors.append(n)
                nb_seen.add(key)
        if desc_len(other) > desc_len(base):
            base.updated_description = other.updated_description or other.original

    rep_map: Dict[str, str] = {}
    new_entities: Dict[str, EntityItem] = {}

    for root, members in clusters.items():
        if len(members) == 1:
            e = ents[members[0]]
            new_entities[e.name] = e
            continue
        members_sorted = sorted(members, key=lambda idx: desc_len(ents[idx]), reverse=True)
        rep_idx = members_sorted[0]
        rep = ents[rep_idx]
        rep_name = rep.name
        for idx in members_sorted[1:]:
            child = ents[idx]
            if child.name == rep_name:
                continue
            merge_two(rep, child)
            rep_map[child.name] = rep_name
        new_entities[rep_name] = rep

    # é‚»å±…é‡å®šå‘
    redirect = rep_map.copy()
    for parent_ent in new_entities.values():
        new_neighbors: List[Neighbor] = []
        seen = set()
        for nb in parent_ent.neighbors:
            tgt = redirect.get(nb.name, nb.name)
            if tgt == parent_ent.name:
                continue
            key = (tgt, nb.snippet)
            if key in seen:
                continue
            seen.add(key)
            new_neighbors.append(Neighbor(name=tgt, snippet=nb.snippet))
        parent_ent.neighbors = new_neighbors

    # è¾“å‡º
    out_entities = {
        e.name: {
            "name": e.name,
            "alias": e.alias,
            "type": e.type,
            "original": e.original,
            "updated_description": e.updated_description,
            "role": e.role,
            "occurrences": [o.__dict__ for o in e.occurrences],
            "neighbors": [n.__dict__ for n in e.neighbors],
        }
        for e in new_entities.values()
    }
    clusters_names = [[names[i] for i in members] for members in clusters.values()]
    decisions_list = [
        {"a": a, "b": b, "is_same": same, "dist": round(float(dist), 6), "reason": reason}
        for a, b, same, dist, reason in merge_decisions
    ]

    t1 = time.time()
    result = {
        "meta": {
            "entities_in": str(entities_path.resolve()),
            "emb_pkl": str(emb_path.resolve()),
            "k_neighbors": int(dcfg["KNN_NEIGHBORS"]),
            "dist_thresh": dist_thresh,
            "workers": workers,
            "mutual_knn": mut_knn,
            "rank_top": rank_top,
            "per_node_cap": per_node_cap,
            "strict_same_role": strict_same_role,
            "use_name_jaccard": use_name_jacc,
            "name_jaccard_min": name_jacc_min,
            "name_mode": name_mode,
            "use_alias_fastpath": use_alias_fast,
            "truncate_desc": trunc_desc,
            "desc_maxlen": desc_maxlen,
            "duration_sec": round(t1 - t0, 2),
        },
        "summary": {
            "pairs_raw": cnt_pairs,
            "pairs_after_filter": len(candidates),
            "pairs_to_llm": len(kept_for_llm),
            "llm_calls": cnt_llm_calls,
            "fallback_calls": rep_merged - rep_fast_merge - cnt_llm_calls,
            "merged_edges": rep_merged,
            "entities_before": len(entities),
            "entities_after": len(new_entities),
        },
        "entities": out_entities,
        "clusters": clusters_names,
        "merge_map": rep_map,
        "decisions": decisions_list,
    }

    out_path.parent.mkdir(parents=True, exist_ok=True)
    with out_path.open("w", encoding=enc) as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    logger.info("\nâ€”â€” å»é‡ä»»åŠ¡æ‘˜è¦ â€”â€”")
    logger.info(f"å€™é€‰å¯¹åŸå§‹æ•°é‡ï¼š{cnt_pairs}")
    logger.info(f"è¿›å…¥ LLM åˆ¤å®šçš„æ•°é‡ï¼š{len(kept_for_llm)}ï¼ˆå·²å¹¶è¡Œå¤„ç†ï¼‰")
    logger.info(f"æˆåŠŸåˆå¹¶çš„å®ä½“å¯¹ï¼š{rep_merged}")
    logger.info(f"å®ä½“æ•°é‡å˜åŒ–ï¼š{len(entities)} -> {len(new_entities)}")
    logger.info(f"æ€»è€—æ—¶ï¼š{t1 - t0:.2f} ç§’")
    logger.info(f"ç»“æœæ–‡ä»¶ï¼š{out_path.resolve()}")


def main():
    # CLI ä»…ä½œæœ€å°è¦†ç›–ï¼ˆå¤šæ•°å‚æ•°å…¨éƒ¨èµ° YAMLï¼‰
    ap = argparse.ArgumentParser(description="Tree-KG å®ä½“å»é‡ï¼ˆçº¯ YAML é…ç½®ç‰ˆï¼‰")
    ap.add_argument("--entities", type=str, default="", help="ï¼ˆå¯é€‰ï¼‰å®ä½“ JSON è·¯å¾„è¦†ç›–")
    ap.add_argument("--emb", type=str, default="", help="ï¼ˆå¯é€‰ï¼‰åµŒå…¥ PKL è·¯å¾„è¦†ç›–")
    ap.add_argument("--out", type=str, default="", help="ï¼ˆå¯é€‰ï¼‰ç»“æœ JSON è·¯å¾„è¦†ç›–")
    args = ap.parse_args()

    # ç°åœ¨ run() é‡Œå®Œå…¨ä» YAML å–ï¼›å¦‚éœ€è¦†ç›–ï¼Œå¯ä»¥è‡ªè¡Œæ”¹ run() æ¥å£ã€‚
    run()


if __name__ == "__main__":
    main()
